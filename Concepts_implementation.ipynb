{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from torchsummary import summary\n",
    "from torch.nn.parameter import Parameter\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 250\n",
    "channels = 4\n",
    "batch_size = 10\n",
    "epochs = 10\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Convolution Layer\n",
    "![Alt text](<image/Screenshot 2023-09-08 at 1.02.46 PM.png>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dynamic_kernel1d(nn.Module):\n",
    "\n",
    "    def __init__(self, input_channels, output_channels, device):\n",
    "        super(dynamic_kernel1d, self).__init__()\n",
    "        self.device = device\n",
    "        self.out_c = output_channels\n",
    "        self.conv1 = nn.Conv2d(input_channels, 32,\n",
    "                               (7, 7), 1, padding='valid')\n",
    "        self.conv2 = nn.Conv2d(32, 32, (7, 7), 1, padding='valid')\n",
    "        self.max_pooling = nn.MaxPool2d(kernel_size=(2, 2), stride=2)\n",
    "        self.soft = nn.Softmax(dim=0)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.max_pooling(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.max_pooling(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.max_pooling(x)\n",
    "\n",
    "        x = x.view(-1)\n",
    "        shape = x.size(0)\n",
    "        v1 = nn.Linear(shape, self.out_c, device=self.device)(x)\n",
    "        h1 = nn.Linear(shape, self.out_c, device=self.device)(x)\n",
    "        v1 = self.soft(v1)\n",
    "        h1 = self.soft(h1)\n",
    "\n",
    "        v1 = v1.unsqueeze(1)  # output x 1\n",
    "        h1 = h1.unsqueeze(0)  # 1 x output\n",
    "\n",
    "        return v1, h1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Filter network\n",
    "![Alt text](<image/Screenshot 2023-09-09 at 11.10.45 AM.png>)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](<image/Screenshot 2023-09-08 at 7.48.12 PM.png>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dynamic_kernel2d(nn.Module):\n",
    "\n",
    "    def __init__(self, input_channels, filters, kernel_size, device):\n",
    "        super(dynamic_kernel2d, self).__init__()\n",
    "        self.device = device\n",
    "        self.filters = filters\n",
    "        self.input_channels = input_channels\n",
    "        self.kernel_size = kernel_size[0]\n",
    "        self.K = 1\n",
    "\n",
    "        self.conv1 = nn.Conv2d(input_channels, input_channels, kernel_size=(7, 7), padding='valid', stride=1,\n",
    "                               groups=input_channels)\n",
    "        self.point1 = nn.Conv2d(input_channels, 64, kernel_size=(1, 1), padding='same', stride=1,\n",
    "                                groups=1)\n",
    "        self.conv2 = nn.Conv2d(64, self.K*64, kernel_size=(7, 7), padding='valid', stride=1,\n",
    "                               groups=64)\n",
    "        self.point2 = nn.Conv2d(self.K*64, 64, kernel_size=(1, 1), padding='same', stride=1,\n",
    "                                groups=1)\n",
    "\n",
    "        self.max_pooling = nn.MaxPool2d(kernel_size=(2, 2), stride=2)\n",
    "        self.soft = nn.Softmax(dim=0)\n",
    "\n",
    "    def linear_layer(self, input, input_shape, output_shape):\n",
    "        return nn.Linear(input_shape, output_shape, device=self.device)(input)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.point1(x)\n",
    "        x = self.max_pooling(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.point2(x)\n",
    "        x = self.max_pooling(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.point2(x)\n",
    "        x = self.max_pooling(x)\n",
    "\n",
    "        # kernel (filters, input_channels, kernel_size, kernel_size)\n",
    "        output_features = self.filters * self.input_channels * \\\n",
    "            self.kernel_size * self.kernel_size\n",
    "        x = x.view(-1)  # Flatten max pooling layer\n",
    "        input_features = x.size(0)  # get flatten size\n",
    "        # fully connected to get desired kernel size\n",
    "        v1 = self.linear_layer(x, input_features, output_features)\n",
    "        # reshape into desired kernel size\n",
    "        v1 = v1.view(self.filters, self.input_channels,\n",
    "                     self.kernel_size, self.kernel_size)\n",
    "\n",
    "        return v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dynamic_filter2d(nn.Module):\n",
    "\n",
    "    def __init__(self, input_channels, kernel_size, filters, device):\n",
    "        super(dynamic_filter2d, self).__init__()\n",
    "        self.device = device\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size[0]\n",
    "        self.input_channels = input_channels\n",
    "        self.K = 1\n",
    "\n",
    "        self.conv1 = nn.Conv2d(input_channels, input_channels, kernel_size=(7, 7), padding='valid', stride=1,\n",
    "                               groups=input_channels)\n",
    "        self.point1 = nn.Conv2d(input_channels, 64, kernel_size=(1, 1), padding='same', stride=1,\n",
    "                                groups=1)\n",
    "        self.conv2 = nn.Conv2d(64, self.K*64, kernel_size=(7, 7), padding='valid', stride=1,\n",
    "                               groups=64)\n",
    "        self.point2 = nn.Conv2d(self.K*64, 64, kernel_size=(1, 1), padding='same', stride=1,\n",
    "                                groups=1)\n",
    "\n",
    "        self.max_pooling = nn.MaxPool2d(kernel_size=(2, 2), stride=2)\n",
    "        self.soft = nn.Softmax(dim=0)\n",
    "\n",
    "    def linear_layer(self, input, input_shape, output_shape):\n",
    "        return nn.Linear(input_shape, output_shape, device=self.device)(input)\n",
    "\n",
    "    def expanding_conv(self, inputs, h, w):\n",
    "        k = h * w * self.filters\n",
    "        return nn.Conv2d(self.input_channels, self.input_channels*k, kernel_size=(1, 1), padding='same', stride=1,\n",
    "                         groups=self.input_channels)(inputs)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        h = inputs.size(2)\n",
    "        w = inputs.size(3)\n",
    "\n",
    "        if True:\n",
    "            # a 2d filter for each pixel of input image for transformation\n",
    "            x = self.conv1(inputs)\n",
    "            x = self.point1(x)\n",
    "            x = self.max_pooling(x)\n",
    "            x = self.conv2(x)\n",
    "            x = self.point2(x)\n",
    "            x = self.max_pooling(x)\n",
    "            x = self.conv2(x)\n",
    "            x = self.point2(x)\n",
    "            x = self.max_pooling(x)\n",
    "            x = self.conv2(x)\n",
    "            x = self.point2(x)\n",
    "            x = self.max_pooling(x)\n",
    "\n",
    "            # kernel (input_height, input_width, input_filters, input_channels, kernel_size, kernel_size)\n",
    "            output_features = self.input_channels * self.kernel_size * self.kernel_size\n",
    "\n",
    "            x = x.view(-1)  # Flatten max pooling layer\n",
    "            input_features = x.size(0)  # get flatten size\n",
    "\n",
    "            # fully connected to get desired kernel size\n",
    "            v1 = self.linear_layer(x, input_features, output_features)\n",
    "            # Reshape to input into conv layer\n",
    "            v1 = v1.view(1, self.input_channels,\n",
    "                         self.kernel_size, self.kernel_size)\n",
    "\n",
    "            # Conv layer to add depth to the kernel to reshape it into desired filter size\n",
    "            v1 = self.expanding_conv(v1, h, w)\n",
    "            # reshape into desired kernel size\n",
    "            v1 = v1.view(h, w, self.filters, self.input_channels,\n",
    "                         self.kernel_size, self.kernel_size)\n",
    "\n",
    "            return v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dynamic_conv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, filter_type, kernel_size, padding, device,\n",
    "                 bias: bool = True, dtype=torch.float32) -> None:\n",
    "        super(dynamic_conv2d, self).__init__()\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        self.filter_type = filter_type\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.empty(\n",
    "                out_channels, **factory_kwargs), requires_grad=True)\n",
    "\n",
    "        assert padding in ['valid', 'same'], \"Either 'same' or 'valid' \"\n",
    "        self.padding = padding\n",
    "\n",
    "        assert filter_type in [\n",
    "            '2d_filter', 'dynamic_filter'], \"Type must be one of : \\n1) 2d_filter \\n2) dynamic_filter\"\n",
    "\n",
    "        if self.filter_type == '2d_filter':\n",
    "            self.filter_network = dynamic_kernel2d(\n",
    "                in_channels, out_channels, kernel_size, device)\n",
    "        else:\n",
    "            self.filter_network = dynamic_filter2d(\n",
    "                in_channels, out_channels, kernel_size, device)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        if self.filter_type == '2d_filter':\n",
    "            kernel = self.filter_network(inputs)\n",
    "        else:\n",
    "            kernel = self.filter_network(inputs)\n",
    "\n",
    "        return F.conv2d(inputs, kernel, self.bias, padding=self.padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.rand((1, 3, 250, 250))\n",
    "inp = inp.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = '2d_filter'  # 2d_filter or dynamic_filter\n",
    "model = dynamic_conv2d(3, 3, i, (3, 3), padding='same', device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(model, (3, 250, 250), batch_size=1, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read input image\n",
    "img = Image.open('IMG_4392.JPG')\n",
    "\n",
    "# convert the input image to torch tensor\n",
    "img = transforms.ToTensor()(img)\n",
    "print(\"Input image size:\", img.size())  # size = [3, 466, 700]\n",
    "\n",
    "# unsqueeze the image to make it 4D tensor\n",
    "img = img.unsqueeze(0)  # image size = [1, 3, 466, 700]\n",
    "# define convolution layer\n",
    "# conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n",
    "\n",
    "img = model(img)\n",
    "print(img.shape)\n",
    "# squeeze image to make it 3D\n",
    "img = img.squeeze(0)  # now size is again [3, 466, 700]\n",
    "\n",
    "# convert image to PIL image\n",
    "img = transforms.ToPILImage()(img)\n",
    "\n",
    "# display the image after convolution\n",
    "img.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Squeeze and Excitation (Channel Attention)\n",
    "![Alt text](<image/Screenshot 2023-09-10 at 1.19.37 PM.png>)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](<image/Screenshot 2023-09-10 at 1.20.10 PM.png>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SqueezeAndExcitation(nn.Module):\n",
    "    def __init__(self, device, ratio=8):\n",
    "        super().__init__()\n",
    "        self.ratio = ratio\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        b = inputs.size(0)\n",
    "        c = inputs.size(1)\n",
    "        x = F.adaptive_avg_pool2d(inputs, 1)  # (batch_size, channels)\n",
    "        # device=self.device\n",
    "        x = nn.Linear(c, c // self.ratio, device=self.device)(x.view(b, c))\n",
    "        x = F.relu(x)\n",
    "        # device=self.device\n",
    "        x = nn.Linear(c // self.ratio, c, device=self.device)(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        x = x.view(b, c, 1, 1)\n",
    "        x = inputs * x  # same size as input\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## changing weights and Convolution\n",
    "![Alt text](<image/Screenshot 2023-09-12 at 11.55.12 AM.png>) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](<image/Screenshot 2023-09-12 at 11.55.17 AM.png>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dynamic_attention(nn.Module):\n",
    "    def __init__(self, device, n_conv, ratio=8):\n",
    "        super().__init__()\n",
    "        self.ratio = ratio\n",
    "        self.device = device\n",
    "        self.n_conv = n_conv\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        b = inputs.size(0)  # batch,channel,h,w\n",
    "        x = inputs.view(b, -1)\n",
    "        channels = x.size(1)\n",
    "        # x = F.adaptive_avg_pool2d(inputs, 1)  # (batch_size, channels, 1, 1)\n",
    "        # device=self.device\n",
    "        x = nn.Linear(channels, channels // self.ratio, device=self.device)(x)\n",
    "        x = F.relu(x)\n",
    "        # device=self.device\n",
    "        x = nn.Linear(channels // self.ratio,\n",
    "                      self.n_conv, device=self.device)(x)\n",
    "        x = torch.softmax(x, dim=1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Test1(nn.Module):\n",
    "    def __init__(self, seed):\n",
    "        super(Test1, self).__init__()\n",
    "        # fix the seed\n",
    "        random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        # init conv\n",
    "\n",
    "        # Assuming these convs are expert filter and are not changing\n",
    "        self.conv1 = nn.Conv2d(3, 3, kernel_size=3, padding=1, bias=True)\n",
    "        self.conv2 = nn.Conv2d(3, 3, kernel_size=3, padding=1, bias=True)\n",
    "        self.conv3 = nn.Conv2d(3, 3, kernel_size=3, padding=1, bias=True)\n",
    "        self.channel_attention = dynamic_attention('cpu', n_conv=3, ratio=1)\n",
    "\n",
    "    def apply_attention(self, attention, weights):\n",
    "        weights *= attention\n",
    "        return weights\n",
    "\n",
    "    def forward(self, x):\n",
    "        attention = self.channel_attention(x)\n",
    "        # print(f\"attention value for each conv :{attention} \")\n",
    "\n",
    "        weight1 = nn.Parameter(self.apply_attention(\n",
    "            attention[0][0], self.conv1.weight.clone()), requires_grad=True)\n",
    "        weight2 = nn.Parameter(self.apply_attention(\n",
    "            attention[0][1], self.conv2.weight.clone()), requires_grad=True)\n",
    "        weight3 = nn.Parameter(self.apply_attention(\n",
    "            attention[0][2], self.conv3.weight.clone()), requires_grad=True)\n",
    "        # print(f\"weight 1 : {weight1} \\n weight 2 : {weight2} \\n weight 1 : {weight3}\")\n",
    "\n",
    "        bias1 = nn.Parameter(self.apply_attention(\n",
    "            attention[0][0], self.conv1.bias.clone()), requires_grad=True)\n",
    "        bias2 = nn.Parameter(self.apply_attention(\n",
    "            attention[0][1], self.conv2.bias.clone()), requires_grad=True)\n",
    "        bias3 = nn.Parameter(self.apply_attention(\n",
    "            attention[0][2], self.conv3.bias.clone()), requires_grad=True)\n",
    "\n",
    "        weight = weight1+weight2+weight3\n",
    "        bias = bias1 + bias2 + bias3\n",
    "        # print(f\"Final weight : {weight}\")\n",
    "        # print(f\"Final bias : {bias}\")\n",
    "\n",
    "        x = F.conv2d(x, weight, bias=bias, stride=1, padding='same')\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # print(f\"out1 weights :{weight} \")\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.rand([1, 3, 128, 128])\n",
    "model = Test1(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = torch.zeros([1, 1, 3, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Shape of scores = (batch,classes, height,widht)\n",
    "loss = metrics(output, targets)\n",
    "# Shape of targets = (batch, height,widht)\n",
    "# calculate to show at final time\n",
    "print(loss)\n",
    "# CAlculate gradient of the loss wrt the parameters\n",
    "# set optimizer's gradients to zero for every batch initially\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "\n",
    "# gradient descent\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual Spatial Attention\n",
    "![Alt text](<image/Screenshot 2023-09-16 at 8.51.03 PM.png>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class soft_mask(nn.Module):\n",
    "    def __init__(self, in_channels) -> None:\n",
    "        super(soft_mask, self).__init__()\n",
    "\n",
    "        self.down = nn.Conv2d(in_channels, in_channels, (3, 3), 2, padding=1)\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode=\"bilinear\")\n",
    "        self.conv_final = nn.Conv2d(in_channels, 1, (1, 1), 1, padding=\"same\")\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.down(inputs)  # half the size\n",
    "        x = self.down(x)  # half the size\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        x = self.upsample(x)\n",
    "\n",
    "        x = self.conv_final(x)\n",
    "        x = F.sigmoid(x)\n",
    "        x = x + 1\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class spatial_attention(nn.Module):\n",
    "    def __init__(self, in_channels) -> None:\n",
    "        super(spatial_attention, self).__init__()\n",
    "\n",
    "        self.soft_mask = soft_mask(in_channels)\n",
    "        self.down = nn.Conv2d(in_channels, in_channels,\n",
    "                              (3, 3), 1, padding=\"same\")\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.down(inputs)\n",
    "        attention = self.soft_mask(inputs)\n",
    "        x = x * attention\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand((2, 64, 128, 128))\n",
    "model = spatial_attention(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = model(a)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BAM & CBAM - Bottleneck Attention (Spatial and Channel)\n",
    "![Alt text](<image/Screenshot 2023-09-17 at 4.31.03 PM.png>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BAM_channel(nn.Module):\n",
    "    def __init__(self, device, ratio=8):\n",
    "        super(BAM_channel, self).__init__()\n",
    "        self.ratio = ratio\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        b = inputs.size(0)\n",
    "        c = inputs.size(1)\n",
    "        x = F.adaptive_avg_pool2d(inputs, 1)  # (batch_size, channels)\n",
    "        # device=self.device\n",
    "        x = nn.Linear(c, c // self.ratio, device=self.device)(x.view(b, c))\n",
    "        x = F.relu(x)\n",
    "        # device=self.device\n",
    "        x = nn.Linear(c // self.ratio, c, device=self.device)(x)\n",
    "        x = F.sigmoid(x)\n",
    "        x = x.view(b, c, 1, 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BAM_spatial(nn.Module):\n",
    "    def __init__(self, in_channels, ratio=8) -> None:\n",
    "        super(BAM_spatial, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels, in_channels //\n",
    "                               ratio, (1, 1), 1, padding=\"same\")\n",
    "        self.conv2 = nn.Conv2d(in_channels // ratio, in_channels //\n",
    "                               ratio, (3, 3), 1, padding=\"same\", dilation=2)\n",
    "        self.final = nn.Conv2d(in_channels // ratio, 1,\n",
    "                               (1, 1), 1, padding=\"same\")\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.final(x)\n",
    "        x = F.sigmoid(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BAM(nn.Module):\n",
    "    def __init__(self, in_channels, ratio, device) -> None:\n",
    "        super(BAM, self).__init__()\n",
    "\n",
    "        self.BAM_channel = BAM_channel(device, ratio)\n",
    "        self.BAM_spatial = BAM_spatial(in_channels, ratio)\n",
    "        self.batch_norm_channel = nn.BatchNorm2d(in_channels)\n",
    "        self.batch_norm_spatial = nn.BatchNorm2d(1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        channel = self.BAM_channel(inputs)\n",
    "        channel = self.batch_norm_channel(channel)\n",
    "        spatial = self.BAM_spatial(inputs)\n",
    "        spatial = self.batch_norm_spatial(spatial)\n",
    "\n",
    "        bam = spatial + channel\n",
    "        bam = F.sigmoid(bam)\n",
    "\n",
    "        inter = bam * inputs\n",
    "\n",
    "        final = inputs + inter\n",
    "\n",
    "        return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential BAM\n",
    "class CBAM(nn.Module):\n",
    "    def __init__(self, in_channels, ratio, device) -> None:\n",
    "        super(CBAM, self).__init__()\n",
    "\n",
    "        self.BAM_channel = BAM_channel(device, ratio)\n",
    "        self.BAM_spatial = BAM_spatial(in_channels, ratio)\n",
    "        self.batch_norm_channel = nn.BatchNorm2d(in_channels)\n",
    "        self.batch_norm_spatial = nn.BatchNorm2d(1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.BAM_channel(inputs)\n",
    "        x = self.batch_norm_channel(x)\n",
    "\n",
    "        x = inputs * x\n",
    "\n",
    "        spatial = self.BAM_spatial(x)\n",
    "        spatial = self.batch_norm_spatial(spatial)\n",
    "\n",
    "        x = spatial * x\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.rand((2, 64, 56, 56))\n",
    "model = CBAM(64, 8, 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = model(inp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class dynamic_kernel2d(nn.Module):\n",
    "\n",
    "#     def __init__(self, output_type, height, width, input_channels, kernel_size,\n",
    "#                  filters, device):\n",
    "#         super(dynamic_kernel2d, self).__init__()\n",
    "#         self.device = device\n",
    "#         self.output_type = output_type\n",
    "#         self.filters = filters\n",
    "#         self.kernel_size = kernel_size[0]\n",
    "#         self.input_channels = input_channels\n",
    "#         self.k = width * height * self.filters\n",
    "\n",
    "#         assert output_type in [\n",
    "#             '2d_filter', 'dynamic_filter'], \"Type must be one of : \\n1) 2d_filter \\n2) dynamic_filter\"\n",
    "\n",
    "#         self.conv1 = nn.Conv2d(input_channels, input_channels, kernel_size=(7, 7), padding='valid', stride=1,\n",
    "#                                groups=input_channels)\n",
    "#         self.point1 = nn.Conv2d(input_channels, 64, kernel_size=(1, 1), padding='same', stride=1,\n",
    "#                                 groups=1)\n",
    "#         self.conv2 = nn.Conv2d(64, 64, kernel_size=(7, 7), padding='valid', stride=1,\n",
    "#                                groups=64)\n",
    "#         self.point2 = nn.Conv2d(64, 64, kernel_size=(1, 1), padding='same', stride=1,\n",
    "#                                 groups=1)\n",
    "#         self.conv3 = nn.Conv2d(input_channels, input_channels*self.k, kernel_size=(1, 1), padding='same', stride=1,\n",
    "#                                groups=input_channels)\n",
    "\n",
    "#         self.max_pooling = nn.MaxPool2d(kernel_size=(2, 2), stride=2)\n",
    "#         self.soft = nn.Softmax(dim=0)\n",
    "\n",
    "#     def linear_layer(self,input,input_shape, output_shape):\n",
    "#         return nn.Linear(input_shape, output_shape, device=self.device)(input)\n",
    "\n",
    "#     def forward(self, inputs):\n",
    "#         h = inputs.size(2)\n",
    "#         w = inputs.size(3)\n",
    "\n",
    "#         if (self.output_type == '2d_filter'):\n",
    "#             # a 2d filter capable to creating transformation\n",
    "#             x = self.conv1(inputs)\n",
    "#             x = self.point1(x)\n",
    "#             x = self.max_pooling(x)\n",
    "#             x = self.conv2(x)\n",
    "#             x = self.point2(x)\n",
    "#             x = self.max_pooling(x)\n",
    "#             x = self.conv2(x)\n",
    "#             x = self.point2(x)\n",
    "#             x = self.max_pooling(x)\n",
    "\n",
    "#             # kernel (filters, input_channels, kernel_size, kernel_size)\n",
    "#             output_features = self.filters * self.input_channels * \\\n",
    "#                 self.kernel_size * self.kernel_size\n",
    "#             x = x.view(-1)  # Flatten max pooling layer\n",
    "#             input_features = x.size(0)  # get flatten size\n",
    "#             v1 = self.linear_layer(x,input_features, output_features)  # fully connected to get desired kernel size\n",
    "#             # reshape into desired kernel size\n",
    "#             v1 = v1.view(self.filters, self.input_channels,\n",
    "#                          self.kernel_size, self.kernel_size)\n",
    "\n",
    "#             return v1\n",
    "\n",
    "#         elif (self.output_type == 'dynamic_filter'):\n",
    "#             # a 2d filter for each pixel of input image for transformation\n",
    "#             x = self.conv1(inputs)\n",
    "#             x = self.point1(x)\n",
    "#             x = self.max_pooling(x)\n",
    "#             x = self.conv2(x)\n",
    "#             x = self.point2(x)\n",
    "#             x = self.max_pooling(x)\n",
    "#             x = self.conv2(x)\n",
    "#             x = self.point2(x)\n",
    "#             x = self.max_pooling(x)\n",
    "#             x = self.conv2(x)\n",
    "#             x = self.point2(x)\n",
    "#             x = self.max_pooling(x)\n",
    "\n",
    "#             # kernel (input_height, input_width, input_filters, input_channels, kernel_size, kernel_size)\n",
    "#             output_features = self.input_channels * self.kernel_size * self.kernel_size\n",
    "\n",
    "#             x = x.view(-1)  # Flatten max pooling layer\n",
    "#             input_features = x.size(0)  # get flatten size\n",
    "\n",
    "#             # fully connected to get desired kernel size\n",
    "#             v1 = self.linear_layer(x,input_features, output_features)\n",
    "#             # Reshape to input into conv layer\n",
    "#             v1 = v1.view(1, self.input_channels,\n",
    "#                          self.kernel_size, self.kernel_size)\n",
    "\n",
    "#             # Conv layer to add depth to the kernel to reshape it into desired filter size\n",
    "#             v1 = self.conv3(v1)\n",
    "#             # reshape into desired kernel size\n",
    "#             v1 = v1.view(h, w, self.filters, self.input_channels,\n",
    "#                          self.kernel_size, self.kernel_size)\n",
    "\n",
    "#             return v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class dynamic_conv2d(nn.Module):\n",
    "#     def __init__(self, filter, padding, device, bias: bool = True) -> None:\n",
    "#         super(dynamic_conv2d, self).__init__()\n",
    "#         factory_kwargs = {'device': device, 'dtype': torch.float32}\n",
    "#         self.filter = filter\n",
    "#         self.out_channels = filter.size(0)\n",
    "#         if bias:\n",
    "#             self.bias = Parameter(torch.empty(\n",
    "#                 self.out_channels, **factory_kwargs))\n",
    "#         self.padding = padding\n",
    "#         self.device = device\n",
    "\n",
    "#         assert padding in ['valid', 'same'], \"Either 'same' or 'valid' \"\n",
    "\n",
    "#     def forward(self, inputs):\n",
    "#         return F.conv2d(inputs, self.filter, self.bias, padding=self.padding)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
